{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040: Assignment 1, Task 1 - Basic ML Classifiers\n",
    "\n",
    "In this task, you are going to implement two classifers and apply them to the  CIFAR-10 dataset: \n",
    "\n",
    "(1) Linear SVM classifier, and\n",
    "\n",
    "(2) Softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules, make sure you have installed all required packages before you start.\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from ecbm4040.cifar_utils import load_data\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 data\n",
    "\n",
    "CIFAR-10 is a widely used dataset which contains 60,000 color images of size 32x32 divided into 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. We are going to use them to create our training set, validation set and test set.\n",
    "\n",
    "See https://www.cs.toronto.edu/~kriz/cifar.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data. You can learn about how to use CIFAR dataset by studing cifar_utils code,\n",
    "# but you don't need to worry about that when doing this task.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# We have vectorized the data (rearranged the storage of images) for you. \n",
    "# That is, we flattened 1×32×32×3 images into 1×3072 Numpy arrays. Number 3 stands for 3 color channels.\n",
    "# The reason we do this is because we can not put 3-D image representations into our model. \n",
    "# This is common practice (flattening images before putting them into the ML model). \n",
    "# Note that this practice may not be used for Convolutional Neural Networks (CNN). \n",
    "# We will later see how we manage the data when used in CNNs in later assignments.\n",
    "\n",
    "# Check the results\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This part will walk you through how to visualize the CIFAR-10 train dataset.\n",
    "# We first find names of 10 categories.\n",
    "f = open('data/cifar-10-batches-py/batches.meta', 'rb')\n",
    "namedict = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "category = namedict['label_names']\n",
    "# We then reshape vectorized data into the image format\n",
    "X = X_train.reshape(50000, 3, 32, 32).transpose(0,2,3,1)\n",
    "\n",
    "print(category)\n",
    "print(X.shape)\n",
    "\n",
    "#Visualizing CIFAR 10 data. We randomly choose 25 images from the train dataset.\n",
    "fig, axes1 = plt.subplots(5,5,figsize=(8,8))\n",
    "for j in range(5):\n",
    "    for k in range(5):\n",
    "        i = np.random.choice(range(len(X)))\n",
    "        axes1[j][k].set_axis_off()\n",
    "        axes1[j][k].imshow(X[i:i+1][0])\n",
    "        axes1[j][k].set_title(category[y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data organization:\n",
    "#    Train data: 49,000 samples from the original train set: indices 1~49,000\n",
    "#    Validation data: 1,000 samples from the original train set: indices 49,000~50,000\n",
    "#    Test data: 1,000 samples from the original test set: indices 1~1,000\n",
    "#    Development data (for gradient check): 100 random samples from the train set: indices 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "X_test = X_test[:num_test, :]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "# Append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Support Vector Machine (SVM) Classifier\n",
    "\n",
    "In this part, you are going to implement a linear SVM classifier. \n",
    "\n",
    "Excellent summaries of SVM methods are presented in ([John Paisley, \"Machine Learning for Data Science,\" SVM_slides](http://www.columbia.edu/~jwp2128/Teaching/W4721/Spring2017/slides/lecture_2-23-17.pdf)) and ([David Sontag, \"Introduction to Machine Learning,\", New York University, Lectures 4,5,6 ](http://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture4.pdf)).\n",
    "\n",
    "**Brief introduction to SVM**\n",
    "\n",
    "Support Vector Mahcine (SVM) is a very important supervised classification model developed in 1992. ([Original paper by Boser et al.](http://dl.acm.org/citation.cfm?id=130401)). It can be used not only for binary classification problems, but also for multiclass classification. As our course reference books points out:\n",
    "> One key innovation associated with support vector machines is the _kernel tricks_.\n",
    "\n",
    "SVM is a __max margin classifier__ that tries to maximize the __margin__ between clusters of data points. The __margin__ between a boundary hyperplane and a cluster is defined as the minimal distance from the points inside the cluster to the boundary. Intuitively speaking, the classification boundary should be as far away from any cluster as possible. \n",
    "\n",
    "![classifier_graph](./img/SVM1.png)\n",
    "\n",
    "The picture above shows what a SVM boundary could look like, in a 2-D plane. Notice that in the left image, the boundary is good enough to distinguish the 2 clusters, but the margins are small (at least one point from a cluster is very close to the boundary). In the image on the right, the boundary separates the 2 clusters, and it is also far from each of the clusters - this is a good SVM boundary. (Image source: Prof. John Paisley, ([_Machine Learning for Data Science_](http://www.columbia.edu/~jwp2128/Teaching/W4721/Spring2017/W4721Spring2017.html)), Columbia University, spring 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we discuss the multi-class linear SVM**\n",
    "\n",
    "Let’s assume a training dataset of images $x_i \\in R^D$, each associated with a label $y_i$. Here $i=1 \\dots N$ and $y_i \\in 1 \\dots K$. That is, we have **N** examples (each with a dimensionality **D**) and **K** distinct categories.\n",
    "\n",
    "We will now define the score function $f: R^D \\to R^K$ that maps the raw image pixels to class scores: $$f(x_i, W, b)=W x_i + b$$\n",
    "where $W$ is of size $K \\times D$ and $b$ is of size $K \\times 1$. \n",
    "\n",
    "Here we will use **bias trick** to represent the two parameters $W,b$ as one by extending the vector $x_i$ with one additional dimension that always holds the constant **1** - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply: $$f(x_i,W)=W x_i$$\n",
    "\n",
    "We then measure our unhappiness with outcomes with a loss function. The SVM loss is set up so that the SVM “wants” the correct class for each image to have a score higher than the incorrect classes by some fixed margin $\\Delta$(here default $\\Delta$ is 1). The score function takes the pixels and computes the vector $f(x_i,W)$ of class scores, which we will abbreviate to $s$, so the score for the j-th class is the j-th element of $s$: $s_j=f(x_i,W)_j$. The Multiclass SVM loss for the i-th example is then formalized as follows: $$L_i=\\sum_{j \\ne y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$\n",
    "\n",
    "An issue with the loss function we presented above is that this set of **W** is not necessarily unique: there might be many similar **W** that correctly classify the examples. We wish to encode some preference for a certain set of weights **W** over others to remove this ambiguity. We can do so by extending the loss function with a **regularization penalty** $R(W)$. The most common regularization penalty is the L2 norm that discourages large weights through an elementwise quadratic penalty over all parameters: $$R(W)= \\sum_k \\sum_d W^2_{k,d}$$\n",
    "\n",
    "Including the regularization penalty completes the full multi-class Support Vector Machine loss, which is made up of two components: the data loss (which is the average loss $L_i$ over all examples) and the regularization loss. That is, the full Multiclass SVM loss becomes: $$L=\\frac{1}{N} \\sum_i L_i + \\lambda R(W)$$\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: The code is given in **./ecbm4040/classifiers/linear_svm.py**, where a naive linear SVM is provided. You need to understand the algorithm and implement a linear SVM using fully vectorized operations with numpy. When you finish the code, run the block below - it will help you verify your result. The loss error should be around 1e-14, while the gradient error should be below 1e-11. Moreover, the vectorized SVM should be much faster than the naive SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.classifiers.linear_svm import svm_loss_naive\n",
    "from ecbm4040.classifiers.linear_svm import svm_loss_vectorized\n",
    "\n",
    "# generate a random SVM weight matrix seeded with small numbers\n",
    "np.random.seed(2345)\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "## naive numpy implementation of SVM\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc-tic))\n",
    "\n",
    "## vectorized numpy implementation of SVM\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))\n",
    "\n",
    "## check the correctness\n",
    "print('*'*100)\n",
    "print('Relative loss error is {}'.format(abs(loss_vec-loss_naive)))\n",
    "grad_err = np.linalg.norm(grad_naive - grad_vec, ord='fro')\n",
    "print('Relative gradient error is {}'.format(grad_err))\n",
    "print('Is vectorized loss correct? {}'.format(np.allclose(loss_naive, loss_vec)))\n",
    "print('Is vectorized gradient correct? {}'.format(np.allclose(grad_naive, grad_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Softmax classifier\n",
    "\n",
    "It turns out that the SVM is one of two commonly seen classifiers. The other popular choice is the Softmax classifier, which has a different loss function. Softmax classifier is binary Logistic Regression classifier's generalization to multiple classes.\n",
    "\n",
    "In the Softmax classifier, the function mapping $f(x_i;W)=W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form: $$L_i= - \\log (\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}})$$\n",
    "\n",
    "The cross-entropy between a “true” distribution **p** and an estimated distribution **q** is defined as: $$H(p, q)=- \\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "Now, let's rewrite the expression of $L_i$: $$L_i= - \\sum_k p_{i,k} \\log (\\frac{e^{f_k}}{\\sum_j e^{f_j}})$$\n",
    "where $p_i=[0, \\dots,1, \\dots, 0]$ contains a single 1 at the $y_i$-th position, $p_{i,k}=p_i[k]$, $p_i \\in [1 \\times K]$\n",
    "\n",
    "**Note:** Numeric stability. When you’re writing code for computing the Softmax function in practice, the intermediate terms $e^{f_{y_i}}$ and $\\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant **C** and push it into the sum, we get the following (mathematically equivalent) expression: $$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}=\\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}=\\frac{e^{f_{y_i}+\\log C}}{\\sum_j e^{f_j+\\log C}}$$\n",
    "\n",
    "A common choice for **C** is to set $\\log C= -\\max_j f_j$\n",
    "\n",
    "In most cases, you also need to consider a bias term $b$ with length D. However, in this experiment, since a bias dimension has been added into the $X$, you can ignore it. \n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: Finish the code in **./ecbm4040/classifiers/softmax.py** and you should implement a softmax layer in three ways. For the first two implementations, we provide the check code for you to verify your code. \n",
    "\n",
    "* Naive method using for-loop\n",
    "* Vectorized method\n",
    "* Softmax in Tensorflow. This step will familiarize you with tensorflow functions. You can refer to the check code.\n",
    "\n",
    "Do not forget the $L2$ regularization term in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.classifiers.softmax import softmax_loss_naive\n",
    "from ecbm4040.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "## generate a random SVM weight matrix of small numbers\n",
    "np.random.seed(2345)\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "## ground truth of loss and gradient\n",
    "W_tf = tf.placeholder(tf.float32, shape=(3073,10))\n",
    "X = tf.placeholder(tf.float32, shape=(None, 3073))\n",
    "y = tf.placeholder(tf.int32, shape=(None,))\n",
    "reg = tf.constant(0.000005)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits= tf.matmul(X, W_tf), labels=tf.one_hot(y,10))\n",
    "loss0 = tf.reduce_mean(cross_entropy) + reg*tf.reduce_sum(W_tf*W_tf)\n",
    "grad0 = tf.gradients(loss0, W_tf)\n",
    "out0 = (loss0, grad0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    tic = time.time()\n",
    "    loss_gt, grad_gt = sess.run(out0, feed_dict={W_tf: W, X: X_dev, y: y_dev})\n",
    "    toc = time.time()\n",
    "\n",
    "## naive softmax in numpy\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc-tic))\n",
    "\n",
    "## vectorized softmax in numpy\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))\n",
    "\n",
    "## Verify your result here - use 'rel_err' for error evaluation.\n",
    "def rel_err(a,b):\n",
    "    return np.mean(abs(a-b))\n",
    "\n",
    "print('*'*100)\n",
    "print('Relative loss error of naive softmax is {}'.format(rel_err(loss_gt,loss_naive)))\n",
    "print('Relative loss error of vectorized softmax is {}'.format(rel_err(loss_gt,loss_vec)))\n",
    "print('Gradient error of naive softmax is {}'.format(rel_err(grad_gt,grad_naive)))\n",
    "print('Gradient error of vectorized softmax is {}'.format(rel_err(grad_gt,grad_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train your classifiers\n",
    "\n",
    "Now you can start to train your classifiers. We are going to use gradient descent algorithm for training, which differs from a usual SVM training process. \n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: The original code is given in **./ecbm4040/classifiers/basic_classifier.py**. You need to finish **train** and **predict** functions in the class **BasicClassifier**. Later, you use its subclasses **LinearSVM** and **Softmax** to train the model seperately and verify your result.\n",
    "\n",
    "**SVM derivations**\n",
    "\n",
    "$$\\nabla_{W_{y_i}} L= -x_i^T(\\sum_{j \\ne y_i}\\mathbb 1(x_i W_j - x_i W_{y_i} + \\Delta > 0)) + 2 \\lambda W_{y_i}$$\n",
    "$$\\nabla_{W_j} L= x_i^T \\mathbb 1(x_i W_j - x_i W_{y_i} + \\Delta > 0) + 2 \\lambda W_j,  j \\ne y_i$$\n",
    "where $\\mathbb 1 (\\cdot)$ is an indicator function, $\\mathbb 1(True)=1$, $\\mathbb 1(False)=0$.\n",
    "\n",
    "**Softmax derivations**\n",
    "\n",
    "$$\\nabla_{W_k} L= - \\frac{1}{N} \\sum_i x_i^T(p_{i,m} - P_m) + 2 \\lambda W_k$$\n",
    "where $P_k= \\frac{e^{f_k}}{\\sum_j e^{f_j}}$\n",
    "\n",
    "In the training section, you are asked to implement Stochastic gradient descent (SGD) optimization method. Pseudo code for SGD is shown below.\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "    ```\n",
    "    w = w - learning_rate * gradient \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Linear SVM + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.classifiers.basic_classifiers import LinearSVM\n",
    "\n",
    "## Linear SVM + SGD\n",
    "classifier = LinearSVM()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train, y=y_train, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance on both the\n",
    "# training set and validation set\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SGD error plot\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Softmax + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.classifiers.basic_classifiers import Softmax\n",
    "\n",
    "## Softmax + SGD\n",
    "classifier = Softmax()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train, y=y_train, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SGD loss curve\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
